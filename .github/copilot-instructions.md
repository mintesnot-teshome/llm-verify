# ğŸ¤– COPILOT AUTO-UPDATE RULE

**Copilot MUST update this file automatically when ANY of the following happens:**

1. **User defines or changes** project domain, stack, database, or scale â†’ Update ğŸ¯ PROJECT IDENTITY
2. **User starts a new task** or completes one â†’ Update ğŸš§ CURRENT FOCUS and âœ… COMPLETED WORK
3. **User makes architectural decisions** â†’ Update ğŸ“‹ IMPORTANT CONTEXT and ğŸ— PATTERNS TO USE
4. **User adds explicit instructions** (e.g., "always do X", "use Y for Z") â†’ Add to ğŸ“œ USER INSTRUCTIONS LOG
5. **User provides credentials or config names** â†’ Add NAME ONLY to ğŸ” CREDENTIALS & CONFIG (âš ï¸ NEVER store values!)
6. **User says "don't do X"** or prohibits something â†’ Add to ğŸš« USER SAID "DON'T DO THIS"
7. **User shares important context** (business rules, constraints, domain knowledge) â†’ Add to ğŸ“‹ IMPORTANT CONTEXT

**After updating, briefly confirm what was changed at the end of the response.**

---

## ğŸ¯ PROJECT IDENTITY

| Field        | Value                                                                                                  |
| ------------ | ------------------------------------------------------------------------------------------------------ |
| **Name**     | LLM Verify                                                                                             |
| **Domain**   | AI model verification & benchmarking â€” detect model fraud (e.g., resold APIs misrepresenting identity) |
| **Stack**    | Python 3.12+ Â· FastAPI Â· Pydantic v2 Â· httpx (async) Â· SQLAlchemy 2.0 (async) Â· Alembic                |
| **Database** | SQLite (dev & prod â€” file-based, zero-config)                                                          |
| **Scale**    | Single-node CLI + web dashboard Â· benchmarks run locally or via CI                                     |
| **Repo**     | `benchmark/`                                                                                           |

---

## ğŸ“œ USER INSTRUCTIONS LOG

| #   | Date       | Instruction                                      |
| --- | ---------- | ------------------------------------------------ |
| 1   | 2026-02-17 | Project bootstrapped with Copilot context system |
|     |            |                                                  |

---

## âœ… COMPLETED WORK

| #   | Date       | Task                                                                   |
| --- | ---------- | ---------------------------------------------------------------------- |
| 1   | 2026-02-17 | Project bootstrap â€” copilot context, settings, gitignore               |
| 2   | 2026-02-17 | Full project scaffolding â€” 30+ files, all layers, 32 prompts           |
| 3   | 2026-02-17 | All 9 unit tests passing                                               |
| 4   | 2026-02-17 | Renamed to LLM Verify, pushed to GitHub                                |
| 5   | 2026-02-17 | Fixed factory: suspect provider now uses Anthropic protocol by default |
| 6   | 2026-02-17 | First live benchmark â€” identity probes vs suspect API (opuscode.pro)   |
| 7   | 2026-02-17 | Confirmed fraud: suspect serves Claude 3.5 Sonnet as Claude Sonnet 4   |
| 8   | 2026-02-17 | Updated README with no-API-key usage guide and red flags doc           |

---

## ğŸš§ CURRENT FOCUS

| Item           | Detail                                                          |
| -------------- | --------------------------------------------------------------- |
| **Working on** | Live testing & analysis of suspect APIs                         |
| **Blockers**   | None                                                            |
| **Next up**    | Test more models (Opus, Haiku), capability suite, web dashboard |

---

## ğŸ” CREDENTIALS & CONFIG

> âš ï¸ **NEVER store actual values here â€” names/keys only!**

| #   | Name                 | Service      | Notes                                |
| --- | -------------------- | ------------ | ------------------------------------ |
| 1   | SUSPECT_API_KEY      | opuscode.pro | Suspect API key â€” Anthropic protocol |
| 2   | SUSPECT_API_BASE_URL | opuscode.pro | https://opuscode.pro/api             |

---

## ğŸš« USER SAID "DON'T DO THIS"

| #   | Date | Prohibition |
| --- | ---- | ----------- |
|     |      |             |

---

## ğŸ“‹ IMPORTANT CONTEXT

- **Core Problem:** Users are being sold API access to models misrepresented as premium models (e.g., Kimi sold as Claude). The system prompt says "Claude" but the underlying model is actually Kimi.
- **Goal:** Build a benchmark suite that can fingerprint AI model behavior to verify true model identity, comparing response patterns, capabilities, and quirks across models.
- **Suspect API (opuscode.pro):** Uses **Anthropic Messages protocol**, NOT OpenAI. Endpoint: `https://opuscode.pro/api/v1/messages`. Auth header: `x-api-key`. Available models: `Opus 4.6`, `Sonnet 4.5`, `Haiku 4.5` (their naming). Default model: `Opus 4.6`.
- **First test result:** Suspect claims to be Claude Sonnet 4 but self-identifies as **claude-3-5-sonnet-20241022** (Claude 3.5 Sonnet). Gave 3 different knowledge cutoffs, mentions "custom proxy server", avg latency 14s.
- **Factory mapping:** `suspect` provider defaults to `anthropic` protocol. Can be overridden via `protocol` field in ModelConfig.
- **Key Features Planned:**
  - Run standardized prompt suites against multiple API endpoints
  - Collect and store structured benchmark results (latency, token usage, response quality)
  - Statistical comparison & fingerprinting to detect model identity
  - Web dashboard to visualize results
  - CLI for running benchmarks in CI/CD

---

## ğŸš¨ HARD RULES

### Security

- âŒ **NEVER** commit secrets, API keys, or tokens to code or config files
- âœ… Use `.env` files (gitignored) and `pydantic-settings` for all secrets
- âœ… Parameterized queries only â€” no string interpolation in SQL
- âœ… Validate all external input with Pydantic models

### Performance

- âœ… Use `async/await` for all I/O (HTTP calls, DB queries, file ops)
- âœ… Use `httpx.AsyncClient` with connection pooling for API calls
- âœ… Use SQLAlchemy async sessions with proper context managers
- âœ… Batch concurrent API calls with `asyncio.gather()` where appropriate

### Architecture

- âœ… Dependency injection via FastAPI `Depends()`
- âœ… Strict separation: handlers â†’ services â†’ repositories â†’ models
- âœ… Each layer has a single responsibility
- âœ… Config is centralized in one place (`src/config.py`)

---

## ğŸ“ CODE STYLE

- **Type hints** on ALL function signatures and return types
- **Docstrings** on all public functions (Google style)
- **Descriptive names** â€” no single-letter variables except `i`, `_` in comprehensions
- **Early returns** to reduce nesting
- **Max 30 lines** per function â€” extract helpers if longer
- **Pydantic models** for all data structures crossing boundaries
- **f-strings** for string formatting
- **`pathlib.Path`** over `os.path`

---

## ğŸ— PATTERNS TO USE

| Pattern                | Usage                                                            |
| ---------------------- | ---------------------------------------------------------------- |
| **Result pattern**     | Return `Result[T, Error]` for operations that can fail           |
| **Service pattern**    | Business logic lives in service classes, not in handlers         |
| **Repository pattern** | DB access abstracted behind repository interfaces                |
| **Adapter pattern**    | Each AI provider gets an adapter implementing a common interface |
| **Factory pattern**    | Create model adapters dynamically from config                    |
| **Strategy pattern**   | Benchmark suites are pluggable strategies                        |

---

## ğŸš« PATTERNS TO AVOID

| Anti-pattern              | Why                                                 |
| ------------------------- | --------------------------------------------------- |
| **God objects**           | Split into focused, single-responsibility classes   |
| **Magic numbers/strings** | Use enums and constants                             |
| **Mutable global state**  | Use DI and explicit passing                         |
| **Generic `utils.py`**    | Create specific modules (`string_helpers.py`, etc.) |
| **Bare `except:`**        | Always catch specific exceptions                    |
| **Print debugging**       | Use `structlog` or `logging`                        |
| **Nested callbacks**      | Use async/await                                     |

---

## ğŸ“ PROJECT STRUCTURE

```
benchmark/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                # Pydantic Settings configuration
â”‚   â”œâ”€â”€ database.py              # SQLAlchemy engine & session setup
â”‚   â”œâ”€â”€ handlers/                # API route handlers (thin layer)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ benchmarks.py
â”‚   â”‚   â””â”€â”€ results.py
â”‚   â”œâ”€â”€ services/                # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ benchmark_runner.py
â”‚   â”‚   â”œâ”€â”€ model_comparator.py
â”‚   â”‚   â””â”€â”€ fingerprint.py
â”‚   â”œâ”€â”€ repositories/            # Database access
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ benchmark_repo.py
â”‚   â”‚   â””â”€â”€ result_repo.py
â”‚   â”œâ”€â”€ models/                  # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ benchmark.py
â”‚   â”‚   â””â”€â”€ result.py
â”‚   â”œâ”€â”€ schemas/                 # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ benchmark.py
â”‚   â”‚   â””â”€â”€ result.py
â”‚   â”œâ”€â”€ adapters/                # AI provider adapters
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # Abstract base adapter
â”‚   â”‚   â”œâ”€â”€ openai_adapter.py
â”‚   â”‚   â”œâ”€â”€ anthropic_adapter.py
â”‚   â”‚   â””â”€â”€ generic_adapter.py   # For OpenAI-compatible APIs
â”‚   â””â”€â”€ prompts/                 # Benchmark prompt suites
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ identity.py          # "Who are you?" probes
â”‚       â”œâ”€â”€ capability.py        # Capability-specific tests
â”‚       â””â”€â”€ fingerprint.py       # Behavioral fingerprinting prompts
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py              # Shared fixtures
â”‚   â”œâ”€â”€ test_benchmark_runner.py
â”‚   â”œâ”€â”€ test_model_comparator.py
â”‚   â””â”€â”€ test_adapters/
â”‚       â””â”€â”€ test_generic_adapter.py
â”œâ”€â”€ alembic/                     # Database migrations
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ”‘ ENVIRONMENT VARIABLES

```env
# === REQUIRED ===
DATABASE_URL=sqlite+aiosqlite:///./benchmarker.db

# === AI PROVIDER API KEYS (add as needed) ===
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=
# SUSPECT_API_KEY=           # The API you're testing/verifying
# SUSPECT_API_BASE_URL=      # Base URL of the suspect API

# === OPTIONAL ===
# LOG_LEVEL=INFO
# BENCHMARK_TIMEOUT=30       # Seconds per API call
# MAX_CONCURRENT_CALLS=5     # Limit parallel API requests
```

---

## ğŸ“š GLOSSARY

| Abbreviation     | Meaning                        |
| ---------------- | ------------------------------ |
| `ctx`            | Context                        |
| `repo`           | Repository                     |
| `svc`            | Service                        |
| `dto`            | Data Transfer Object           |
| `handler`        | API route handler (controller) |
| `adapter`        | AI provider adapter            |
| `cfg` / `config` | Configuration                  |
| `db`             | Database                       |
| `req` / `res`    | Request / Response             |
| `bench`          | Benchmark                      |
| `fp`             | Fingerprint                    |

---

## âœ… TESTING

- **Test alongside code** â€” tests mirror `src/` structure
- **Mock all externals** â€” API calls, database, file system
- **Cover edge cases** â€” empty inputs, timeouts, malformed responses
- **Target 80% coverage** minimum
- **Use `pytest`** with `pytest-asyncio` for async tests
- **Fixtures in `conftest.py`** â€” shared test data and mocks
- **Test naming:** `test_<function>_<scenario>_<expected>` (e.g., `test_run_benchmark_timeout_raises_error`)
- **Use `httpx.AsyncClient`** for integration testing FastAPI endpoints
