# ðŸ” LLM Verify â€” AI Model Fraud Detector & LLM Fingerprinting Toolkit

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Detect fake AI APIs** â€” Verify if an LLM API is actually serving the model it claims. Catch resellers who sell you "Claude" or "ChatGPT" but secretly serve Kimi, LLaMA, or other cheaper models behind a system prompt.

## The Problem

AI API resellers are committing **model fraud**: they sell access to premium models like Claude or ChatGPT, but behind the scenes, they use a cheaper model with a system prompt like _"You are Claude, made by Anthropic."_ You're paying premium prices for a knockoff.

**LLM Verify** catches this by running behavioral fingerprinting benchmarks â€” a suite of prompts designed to reveal a model's true identity through its response patterns, not just what it _says_ it is.

### Key Features

- ðŸ§¬ **Behavioral Fingerprinting** â€” Identify models by how they respond, not what they claim
- ðŸ†š **Side-by-Side Comparison** â€” Compare suspect APIs against verified baselines
- ðŸŽ¯ **32 Forensic Prompts** â€” Identity probes, capability tests, and style analysis
- ðŸ“Š **Multi-Dimensional Scoring** â€” Latency, token usage, vocabulary, formatting patterns
- âš¡ **Async & Fast** â€” Concurrent API calls with configurable rate limiting
- ðŸ”Œ **Any OpenAI-Compatible API** â€” Works with any endpoint that speaks the OpenAI protocol

## Quick Start

```bash
# 1. Create virtual environment
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate  # Linux/macOS

# 2. Install dependencies
pip install -e ".[dev]"

# 3. Copy environment config
cp .env.example .env
# Edit .env with your API keys

# 4. Run the API server
uvicorn src.main:app --reload

# 5. Run tests
pytest
```

## API Endpoints

| Method | Endpoint                           | Description                        |
| ------ | ---------------------------------- | ---------------------------------- |
| GET    | `/health`                          | Health check                       |
| POST   | `/api/v1/benchmarks/`              | Start a new benchmark run          |
| GET    | `/api/v1/benchmarks/`              | List all benchmark runs            |
| GET    | `/api/v1/benchmarks/{id}`          | Get a specific benchmark run       |
| GET    | `/api/v1/results/{run_id}`         | Get results for a run              |
| POST   | `/api/v1/results/compare`          | Compare two runs (fraud detection) |
| GET    | `/api/v1/results/{id}/fingerprint` | Generate behavioral fingerprint    |

## How It Works

1. **Run benchmarks** against a trusted model (e.g., real Claude API) â†’ baseline
2. **Run same benchmarks** against the suspect API
3. **Compare** the two runs â€” the system analyzes:
   - Response latency patterns
   - Response length & style
   - Token usage
   - Error rates
   - Vocabulary & formatting fingerprints
4. **Get verdict:** MATCH, MISMATCH, or INCONCLUSIVE

## Project Structure

```
src/
â”œâ”€â”€ adapters/      # AI provider API clients (OpenAI, Anthropic, generic)
â”œâ”€â”€ handlers/      # FastAPI route handlers
â”œâ”€â”€ models/        # SQLAlchemy ORM models
â”œâ”€â”€ prompts/       # Benchmark prompt suites (identity, capability, fingerprint)
â”œâ”€â”€ repositories/  # Database access layer
â”œâ”€â”€ schemas/       # Pydantic request/response models
â”œâ”€â”€ services/      # Business logic (runner, comparator, fingerprinting)
â”œâ”€â”€ config.py      # Centralized settings
â”œâ”€â”€ database.py    # Async SQLAlchemy setup
â””â”€â”€ main.py        # FastAPI app entry point
```

## License

MIT
